{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis is an implementation for [Attention is all you need](https://arxiv.org/abs/1706.03762) and references from [Pytorch Seq2Seq - Transformer](https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb), [Harvard's Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) and this brilliant blog -[Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) . \n\nIt is implemented as an excerise to gain a deeper understanding of Transformer models by exploring its internal layers and implementing the same for translation task. The notebook is a followup to the first 3 notebooks where last implementation was a CNN Encoder-Decoder model with Attention.\n\nDataset used - [English - French Translations](https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset)","metadata":{}},{"cell_type":"markdown","source":"### Model architecture\n\nThe Transformer is an Encoder-Decoder Model ->\n\nBasic Model flow = Input -> **Encoder** -> **Decoder** -> Output\n\n    Encoder =\n        Stack of 6 Encoders =\n            Each Encoder =\n                EncoderLayer(\n                    Self-Attention -> Feed-Forward \n                    (each word embedding has its own parallel processing independently. only self attention has dependency on other words)\n               )\n\nThe first encoder layer also contains the embedding layer for word along with positional embedding which it receives as input. All the other encoder layers receive the output of the previous layer as input. We'll see the embedding layer in detail later.\n\n    Decoder =\n        Stack of 6 Decoders =\n            Each Decoder =\n                DecoderLayer(\n                    Self-Attention -> Encoder-Decoder-Attention -> Feed-Forward\n               )  \n               \nNote - Each internal representation is 512 in dim","metadata":{}},{"cell_type":"markdown","source":"## Encoder\n\n#### Embedding layer in first encoder layer\n\nEmbedding layer -> WordEmbedding\nPostitional Encoding (formula mentioned in paper) -> PositionalEncoding (for each word)\n\nInput to Encoder = WordEmbedding + PostitionalEncoding (each is of 512 size, summed element-wise) -> output(512 in length)\n\n\n### Encoder Layer\n\nNote - Each encoder layer has the same architecture\n\nConsider a sentence as a matrix of **sent_len X 512** (where sent_len is length of sentence and 512 is the vector representation size of each word. For eg- input layer has the word embeddings)\n\nRepresent input matrix by **inp_mat**\n\nEach encoder layer receives this dim vector as input.\n\n#### Self-Attention\n* For calculating self-attention, 3 weight matrices are maintained for Query(Wq), Key(Wk) and Value(Wv). Each of size **512 X 64** in the paper \n* **inp_mat** is multiplied with each weight matrix to create respective Query(Qv), Key(Kv) and Value(Vv) vectors. Each vector of size - **sent_len X 64**\n* Calculate self-attention score for each word against all other words.\n* Self attention score calculation - \n\n    For each word ->\n        Multiply Qv (query) of the word with other words KvT (key vector transpose)\n        -> returns the score vector(Sv) of size -> **sent_len X sent_len**\n        -> by intution ( the size sent_len X sent_len means a value for each word in the sentence w.r.t all the other words)\n        -> eg - sentence -> Good morning \n        -> Sv could be =           Good  morning\n                         Good       [1.5,  2.7\n                         morning    3.5,   0.3]\n\n\n* Divide Sv by 8 (square root of key vector size - 64). For more stable gradients as per paper.\n* Pass Sv through softmax operation to normalize the scores and make them add up to 1.\n* Matrix **Sv(size - sent_len X sent_len)** is multiplied with Matrix **Vv(size - sent_len X 64)**. This produces the output of the self attention layer. Output size - **sent_len X 64\n* By intitution, in the last step -> multiplying each words value vector by the current word's attention score for that vector highlights important words as their attention score would be more and diminshing other words with lower attention score as they get multiplied  with values like 0.0001.\n* Let's call this output matrix Z (size - **sent_len X 64**)\n\n##### Multi-headed attention\n* Following from the previous step -> instead of a single set of weight matrices (Wq, Wk, Wv), consider multiple sets of these matrices (in paper 8 sets of query, key and value matrices are used).\n* Now each of these sets are used separately to process the self-attention flow listed above and produce their respective Z matrix as output -> (in paper Z1, Z2....Z8 -> 8 matrices).\n* This is called multi-headed attention. This is helpful for the model to look at differnt patterns in the sentences and maybe consider different sub-sentences lengths in different heads.\n\n##### Final Processing of Self-Attention\n* Concatenate all Z matrices -> along the column -> matrix of size - sent_len X (64X8) = **sent_len X 512**\n* Multiply with another weight matrix W0 (this matrix is also learned along with the model) -> output O => size - **sent_len X 512**\n\n#### Feed-Forward\n* Now the output **O from Self-Attention** is passed to feed forward network. Each word embedding goes through a separate feed forward network. **So, the no. of feed forward networks = sent_len**.\n* **Feed forward input 1 X 512**  -> **Feed forward output 1 X 512**\n* All word outputs together form an output Fi matrix of size -> **sent_len X 512** -> which is the output of Encoder layer i. This will serve as the input of the next encoder layer and is of the same dimension as input to the encoder layer.\n* The size is kept constant across layers in transformer.\n\n#### Residuals (LayerNorm - Add & Normalize)\n* Output of each Sublayer(Self-Attention & Feed-Forward) of Encoder layer is summed element wise with the input to that layer and normalized.\n* For eg - Oi output from Self-Attention is added with Embedding in the first encoder layer and output Ei-1 in the other encoder layers and normalized. Fi output from feed-forward layer is added with Oi output from Self-Attention just before it that has been normalized and the sum is further normalized.\n\n(ignoring embedding here in the first case for generic representation)\nEncoder single layer process ->\n\n    Input inp (sent_len X 512) ->\n        Self-Attention ->\n            W = 8 sets of Wq, Wk, Wv\n            Z = []\n            for (Wk, Wq, Wv) in W:\n                Qv = inp X Wq (sent_len X 64)\n                Kv = inp X Wk (sent_len X 64)\n                Vv = inp X Wv (sent_len X 64)\n\n                Sv = Qv X KvT (sent_len X sent_len)\n                Sv = Sv/8  (for stable gradients)\n                Sv = Softmax(Sv) (for normalizing and making sure all values sum upto 1)\n                Zi = Sv X Vv (sent_len X 64)\n                Z.append(Z)\n                \n             Zconcat = Z1.concat(Z2).concat(Z3)....concat(Z8)   (sent_len X 512)\n             \n             FI = Zconcat X Wo (sent_len X 512)\n             \n             FI = FI + inp (residual adding)\n             FI = norm(FI)\n        \n        Feed-Forward ->\n             FO = []\n             for i in parallel_process(sent_len):\n                 FOi = FI[i] -> feed-forward layer (1 X 512)\n                 FO.concat(FOi)\n                 \n             FO (sent_len X 512)\n             FO = FO + FI (residual summing)\n             FO = norm(FO) (layer norm)\n             \n        Encoder layer output = FO\n             \n\n","metadata":{}},{"cell_type":"markdown","source":"## Decoder\n\nThe decoder is fed as input the target sentence tokens as input along with the ouptut of the top Encoder layer. It produces as output in a single run the id of the token generated in vocabulary and it keeps producing the tokens until a special token, generally EOS token is reached.\n\n#### Embedding layer in first decoder layer for target sentence\n\nEmbedding layer -> WordEmbedding\nPostitional Encoding (formula mentioned in paper) -> PositionalEncoding (for each word)\n\nInput to Decoder = WordEmbedding + PostitionalEncoding (each is of 512 size, summed element-wise) -> output(512 in length)\n\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence.** This is done by masking future positions (setting them to -inf)** before the softmax step in the self-attention calculation.\n\n\n### Decoder Layer\n\nNote - Each decoder layer has the same architecture\n\nTarget sentence is a matrix of **sent_len X 512** after passing through the embedding layer(where sent_len is length of sentence and 512 is the vector representation size of each word).\n\nRepresent target sent matrix by **trg_mat**\n\nEach decoder layer receives this dim vector as input.\n\n#### Self-Attention\n* Self-Attention operates similarly to how it operates in encoder layer. Only difference is that in the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n\n* Output of self attention size - **sent_len X 512**\n\n#### Encoder-Decoeder-Attention\n\n* Operates in the same way as self-attention except the Keys and Values vectors are constructed from the Encoder output(sent_len X 512) and the query vector is constructed from the previous layer's output matrix (sent_len X 512).\n\n### Final Linear Layer\n* Output of the last decoder layer is passed through a linear layer which takes a vector of size 512 (float values) and maps it to vector of vocab size (1 X target_vocab_size). \n* Softmax is applied to covert these values to probabilities adding upto 1. \n* The index with the highest probability is considered as the index of the predicted word from the vocabulary. \n* This word is fed along with the previous generated words to the decoder for the next run.\n\n(ignoring embedding here in the first case for generic representation)\nDecoder single layer process ->\n\n    Input - trg_mat (sent_len X 512, future tokens will be masked during attention process)\n          - enc_out (sent_len X 512, to be used in encoder-decoder attention layer)\n    ->\n        Self-Attention ->\n            W = 8 sets of Wq, Wk, Wv\n            Z = []\n            for (Wk, Wq, Wv) in W:\n                Qv = trg_mat X Wq (sent_len X 64)\n                Kv = trg_mat X Wk (sent_len X 64)\n                Vv = trg_mat X Wv (sent_len X 64)\n\n                Sv = Qv X KvT (sent_len X sent_len)\n                Sv = Sv/8  (for stable gradients)\n                Sv = mask(Sv) # future tokens are masked by setting probability to -inf\n                Sv = Softmax(Sv) (for normalizing and making sure all values sum upto 1)\n                Zi = Sv X Vv (sent_len X 64)\n                Z.append(Z)\n                \n             Zconcat = Z1.concat(Z2).concat(Z3)....concat(Z8)   (sent_len X 512)\n             \n             EDI = Zconcat X Wo (sent_len X 512)\n             \n             EDI = EDI + trg_mat (residual adding)\n             EDI = norm(EDI)\n             \n        Encoder-Decoder-Attention ->\n            W = 8 sets of Wq, Wk, Wv\n            Z = []\n            for (Wk, Wq, Wv) in W:\n                Qv = EDI X Wq (sent_len X 64)\n                Kv = enc_out X Wk (sent_len X 64)\n                Vv = enc_out X Wv (sent_len X 64)\n\n                Sv = Qv X KvT (sent_len X sent_len)\n                Sv = Sv/8  (for stable gradients)\n                Sv = mask(Sv) # future tokens are masked by setting probability to -inf\n                Sv = Softmax(Sv) (for normalizing and making sure all values sum upto 1)\n                Zi = Sv X Vv (sent_len X 64)\n                Z.append(Z)\n                \n             Zconcat = Z1.concat(Z2).concat(Z3)....concat(Z8)   (sent_len X 512)\n             \n             FI = Zconcat X Wo (sent_len X 512)\n             \n             FI = FI + EDI (residual adding)\n             FI = norm(FI)\n        \n        Feed-Forward ->\n             FO = []\n             for i in parallel_process(sent_len):\n                 FOi = FI[i] -> feed-forward layer (1 X 512)\n                 FO.concat(FOi)\n                 \n             FO (sent_len X 512)\n             FO = FO + FI (residual summing)\n             FO = norm(FO) (layer norm)\n             \n        Decoder layer output = FO","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}