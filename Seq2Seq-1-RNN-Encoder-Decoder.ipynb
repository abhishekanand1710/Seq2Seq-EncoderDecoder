{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis is an implementation for Sequence to Sequence Encoder - Decoder Model using LSTM trained for the task of English-French translations. It is based on the paper - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) and the notebook [Pytorch Seq2Seq](https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb). \n\nIt is implemented as an excerise to gain a deeper understanding of Encoder - Decoder models and build on it to explore advancements on the same.\n\nDataset used - [English - French Translations](https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset)","metadata":{}},{"cell_type":"markdown","source":"## Model training walkthrough\n\nThe idea behind such a model is to use the Encoder to encode the source sentence to a context vector, which the Decoder uses to generate the target sentence. \n\nEncoder Process - \n* Source sentence is split up into tokens and vocabularized. The sentences' tokens are passed into the model one by one using their indices from the vocab as input.\n* The input is fed to an embedding layer which learns to generate an embedding vector (E) for the word. (A pre-trained embedding can be utilised here to improve accuracy and reduce training time by freezing the layer)\n* E is fed into an RNN unit (either Vanilla RNN, LSTM or GRU) which also takes the hidden state of the previous token run as input (for the first token, it can be initialized as a randome vector or a tensor of 0s).\n* E vector and the hidden state of the previous token pass (Hi-1) is used as input by the RNN unit to generate output and hidden state.\n* The generated hidden state serves as the input to next token run, along with the next token's E vector.\n* The context vector from the encoder is the output(hidden state) of the final token run from the source sentence.\n* The context vector is used as the input to the decoder.\n\nexample pseudocode for one sentence pass through encoder - \n\n<code>\n    vocab = build_vocab(source_sentence)\n    sentence_tensor = [vocab[tokenize(word)] for word in source_sentence.split(' ')]\n    hidden = 0\n    for word_tensor in sentence_tensor:\n        emb_vector = embed(word_tensor)\n        hidden, output = rnn_unit(emb_vector, hidden) # hidden = output for 1 layer rnn\n    context = hidden\n</code>\n\n<br/>\nDecoder process - \n* Target sentence is split up into tokens and vocabularized. The sentences' tokens are passed into the model one by one using their indices from the vocab as input.\n* The input is fed to an embedding layer which learns to generate an embedding vector (E) for the word. (A pre-trained embedding can be utilised here to improve accuracy and reduce training time by freezing the layer)\n* E is fed into an RNN unit (either Vanilla RNN, LSTM or GRU) which also takes the hidden state of the previous token run as input (for the first token, it is the context vector returned from the encoder).\n* The RNN unit uses E and hidden state from previous token as input and generates output vector and hidden state for the next token pass.\n* The output vector is passed through a linear layer with output dimension as the vocab size. Argmax of linear layer output is taken to get the index of token generated in vocab.\n* This token is then later used as input for the next token pass. \n* In case of teacher forcing approach, actual target token is used instead of generated token for next pass.\n\nexample pseudocode for one sentence pass through decoder - \n\n<code>\n    vocab = build_vocab(target_sentence)\n    sentence_tensor = [vocab[tokenize(word)] for word in target_sentence.split(' ')]\n    hidden = context\n    word_tensor = sentence_tensor[0]\n    predicted_sentence = []\n    for i in range(len(sentence_tensor)):\n        emb_vector = embed(word_tensor)\n        hidden, output = rnn_unit(emb_vector, hidden) # hidden = output for 1 layer rnn\n        prediction = argmax(linear_layer(output))\n        word_tensor = prediction\n        predicted_sentence.append(prediction)\n</code>\n<br/>","metadata":{}},{"cell_type":"markdown","source":"### Module imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport spacy\nimport random\nfrom torchtext.data.utils import get_tokenizer\nimport torch\nimport torchtext\nfrom collections import Counter\nfrom torchtext.vocab import vocab\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\ntorch.cuda.empty_cache()\n\nimport math\nimport time\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-10T09:36:04.327013Z","iopub.execute_input":"2023-04-10T09:36:04.327710Z","iopub.status.idle":"2023-04-10T09:36:26.983112Z","shell.execute_reply.started":"2023-04-10T09:36:04.327669Z","shell.execute_reply":"2023-04-10T09:36:26.981846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 97\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:36:26.985283Z","iopub.execute_input":"2023-04-10T09:36:26.986346Z","iopub.status.idle":"2023-04-10T09:36:26.994768Z","shell.execute_reply.started":"2023-04-10T09:36:26.986305Z","shell.execute_reply":"2023-04-10T09:36:26.993776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downloading Spacy models for use as tokenizers","metadata":{}},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\n!python -m spacy download fr_core_news_sm","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-10T09:36:26.996259Z","iopub.execute_input":"2023-04-10T09:36:26.996697Z","iopub.status.idle":"2023-04-10T09:37:17.183943Z","shell.execute_reply.started":"2023-04-10T09:36:26.996657Z","shell.execute_reply":"2023-04-10T09:37:17.182733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data import\nReading only 50000 rows for demonstration purposes. Training RNN is slow as it it takes input tokens one by one and I am limited by the compute available.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/en-fr-translation-dataset/en-fr.csv', nrows=50000)\ndata = data.dropna().drop_duplicates()\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:37:17.189081Z","iopub.execute_input":"2023-04-10T09:37:17.189431Z","iopub.status.idle":"2023-04-10T09:37:17.267595Z","shell.execute_reply.started":"2023-04-10T09:37:17.189396Z","shell.execute_reply":"2023-04-10T09:37:17.266636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing the tokenizers","metadata":{}},{"cell_type":"code","source":"fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\nen_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:37:17.268824Z","iopub.execute_input":"2023-04-10T09:37:17.269306Z","iopub.status.idle":"2023-04-10T09:37:23.910411Z","shell.execute_reply.started":"2023-04-10T09:37:17.269266Z","shell.execute_reply":"2023-04-10T09:37:23.909337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting train, test and val datasets\nTrain - 85%, Val - 10%, Test - 5%","metadata":{}},{"cell_type":"code","source":"val_frac = 0.1\ntest_frac = 0.05\n\nval_split_idx = int(len(data)*val_frac)\ntest_split_idx = int(len(data)*(val_frac + test_frac))\n\ndata_idx = list(range(len(data)))\nnp.random.shuffle(data_idx)\n\nval_idx, test_idx, train_idx = data_idx[:val_split_idx], data_idx[val_split_idx:test_split_idx], data_idx[test_split_idx:]\n\ndf_train = data.iloc[train_idx].reset_index().drop('index',axis=1)\ndf_val = data.iloc[val_idx].reset_index().drop('index',axis=1)\ndf_test = data.iloc[test_idx].reset_index().drop('index',axis=1)\n\nprint('Length of train set - ', len(df_train))\nprint('Length of validation set - ', len(df_val))\nprint('Length of test set - ', len(df_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:37:23.913011Z","iopub.execute_input":"2023-04-10T09:37:23.913823Z","iopub.status.idle":"2023-04-10T09:37:23.931009Z","shell.execute_reply.started":"2023-04-10T09:37:23.913779Z","shell.execute_reply":"2023-04-10T09:37:23.929733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_vocab(data, source_tokenizer, target_tokenizer):\n    en_counter = Counter()\n    fr_counter = Counter()\n    translations = data.values.tolist()\n    for translation in translations:\n        en_counter.update(source_tokenizer(translation[0]))\n        fr_counter.update(target_tokenizer(translation[1]))\n    return vocab(en_counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'], min_freq=2), vocab(fr_counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'], min_freq=2)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:37:23.933090Z","iopub.execute_input":"2023-04-10T09:37:23.933542Z","iopub.status.idle":"2023-04-10T09:37:23.944610Z","shell.execute_reply.started":"2023-04-10T09:37:23.933495Z","shell.execute_reply":"2023-04-10T09:37:23.943653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building separate vocabs for English & French","metadata":{}},{"cell_type":"code","source":"en_vocab, fr_vocab = build_vocab(df_train, en_tokenizer, fr_tokenizer)\nen_vocab.set_default_index(en_vocab['<unk>'])\nfr_vocab.set_default_index(fr_vocab['<unk>'])","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:34.239513Z","iopub.execute_input":"2023-04-10T09:39:34.240443Z","iopub.status.idle":"2023-04-10T09:39:35.850879Z","shell.execute_reply.started":"2023-04-10T09:39:34.240386Z","shell.execute_reply":"2023-04-10T09:39:35.849833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_process(data):\n    translations = data.values.tolist()\n    pairs = []\n    for translation in translations:\n        en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(translation[0])][::-1],\n                            dtype=torch.long)\n        fr_tensor = torch.tensor([fr_vocab[token] for token in fr_tokenizer(translation[1])],\n                            dtype=torch.long)\n        pairs.append((en_tensor, fr_tensor))\n    return pairs","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:36.445245Z","iopub.execute_input":"2023-04-10T09:39:36.446207Z","iopub.status.idle":"2023-04-10T09:39:36.453769Z","shell.execute_reply.started":"2023-04-10T09:39:36.446151Z","shell.execute_reply":"2023-04-10T09:39:36.452593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data_process(df_train)\nval_data = data_process(df_val)\ntest_data = data_process(df_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:38.050136Z","iopub.execute_input":"2023-04-10T09:39:38.050522Z","iopub.status.idle":"2023-04-10T09:39:38.887795Z","shell.execute_reply.started":"2023-04-10T09:39:38.050486Z","shell.execute_reply":"2023-04-10T09:39:38.886687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\nKeeping a small batch size due to limitation of GPU size.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATCH_SIZE = 8\nPAD_IDX = en_vocab['<pad>']\nBOS_IDX = en_vocab['<bos>']\nEOS_IDX = en_vocab['<eos>']","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:39.757312Z","iopub.execute_input":"2023-04-10T09:39:39.758109Z","iopub.status.idle":"2023-04-10T09:39:39.763798Z","shell.execute_reply.started":"2023-04-10T09:39:39.758065Z","shell.execute_reply":"2023-04-10T09:39:39.762734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_batch(data_batch):\n    en_batch, fr_batch = [], []\n    for (en_item, fr_item) in data_batch:\n        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0).to(device))\n        fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0).to(device))  \n        \n    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n    return en_batch, fr_batch","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:40.395869Z","iopub.execute_input":"2023-04-10T09:39:40.396247Z","iopub.status.idle":"2023-04-10T09:39:40.405453Z","shell.execute_reply.started":"2023-04-10T09:39:40.396212Z","shell.execute_reply":"2023-04-10T09:39:40.404138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:40.821230Z","iopub.execute_input":"2023-04-10T09:39:40.821881Z","iopub.status.idle":"2023-04-10T09:39:40.827705Z","shell.execute_reply.started":"2023-04-10T09:39:40.821844Z","shell.execute_reply":"2023-04-10T09:39:40.826653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoder\n* input_dim - source sentence vocab size\n* emb_dim - output size of embedding layer\n* hid_dim - Output size of hidden vector and output vector generated by RNN unit (LSTM in this case)\n* n_layers - No. of RNN layers(RNN units) to stack or which the token passes through ( In case of multiple layers, the final context vector has final hidden vectors generated from each layer stacked on top of each other)\n* dropout - percentage of droput to be used to avoid overfitting\n","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        \n        #src = [src len, batch size]\n        \n        embedded = self.dropout(self.embedding(src))\n        \n        #embedded = [src len, batch size, emb dim]\n        \n        outputs, (hidden, cell) = self.lstm(embedded)\n        \n        #outputs = [src len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #outputs are always from the top hidden layer\n        \n        return hidden, cell","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:41.511144Z","iopub.execute_input":"2023-04-10T09:39:41.512045Z","iopub.status.idle":"2023-04-10T09:39:41.520495Z","shell.execute_reply.started":"2023-04-10T09:39:41.511993Z","shell.execute_reply":"2023-04-10T09:39:41.519176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder\n* output_dim - target sentence vocab size\n* emb_dim - output size of embedding layer\n* hid_dim - Output size of hidden vector and output vector generated by RNN unit (LSTM in this case)\n* n_layers - No. of RNN layers(RNN units) to stack or which the token passes through (in this example we keep the number of layers same in case of encoder and decoder, so the generated context vector can be directly used in decoder without the need of any summartion or passing through a linear layer for transformation)\n* dropout - percentage of droput to be used to avoid overfitting\n","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.output_dim = output_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        \n        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n        \n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, cell):\n        \n        #input = [batch size]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #n directions in the decoder will both always be 1, therefore:\n        #hidden = [n layers, batch size, hid dim]\n        #context = [n layers, batch size, hid dim]\n        \n        input = input.unsqueeze(0)\n        \n        #input = [1, batch size]\n        \n        embedded = self.dropout(self.embedding(input))\n        \n        #embedded = [1, batch size, emb dim]\n                \n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        \n        #output = [seq len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #seq len and n directions will always be 1 in the decoder, therefore:\n        #output = [1, batch size, hid dim]\n        #hidden = [n layers, batch size, hid dim]\n        #cell = [n layers, batch size, hid dim]\n        \n        prediction = self.fc_out(output.squeeze(0))\n        \n        #prediction = [batch size, output dim]\n        \n        return prediction, hidden, cell","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:41.838460Z","iopub.execute_input":"2023-04-10T09:39:41.838815Z","iopub.status.idle":"2023-04-10T09:39:41.851885Z","shell.execute_reply.started":"2023-04-10T09:39:41.838782Z","shell.execute_reply":"2023-04-10T09:39:41.850896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model - \nContains the entire process explained earlier. As input it takes a batch of sentences and passes it through encoder to generate a context vector tensor for the entire batch. Then token by token it passes the sentence batch to decoder to generate output tokens.","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            \"Hidden dimensions of encoder and decoder must be equal!\"\n        assert encoder.n_layers == decoder.n_layers, \\\n            \"Encoder and decoder must have equal number of layers!\"\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src len, batch size]\n        #trg = [trg len, batch size]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        \n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(src)\n        \n        #first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            \n            #insert input token embedding, previous hidden and previous cell states\n            #receive output tensor (predictions) and new hidden and cell states\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #get the highest predicted token from our predictions\n            top1 = output.argmax(1) \n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            input = trg[t] if teacher_force else top1\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:42.168236Z","iopub.execute_input":"2023-04-10T09:39:42.168989Z","iopub.status.idle":"2023-04-10T09:39:42.178936Z","shell.execute_reply.started":"2023-04-10T09:39:42.168947Z","shell.execute_reply":"2023-04-10T09:39:42.177418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(en_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:42.429338Z","iopub.execute_input":"2023-04-10T09:39:42.430388Z","iopub.status.idle":"2023-04-10T09:39:42.437294Z","shell.execute_reply.started":"2023-04-10T09:39:42.430346Z","shell.execute_reply":"2023-04-10T09:39:42.435984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(fr_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:43.498247Z","iopub.execute_input":"2023-04-10T09:39:43.498660Z","iopub.status.idle":"2023-04-10T09:39:43.506929Z","shell.execute_reply.started":"2023-04-10T09:39:43.498623Z","shell.execute_reply":"2023-04-10T09:39:43.505788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing the model","metadata":{}},{"cell_type":"code","source":"INPUT_DIM = len(en_vocab)\nOUTPUT_DIM = len(fr_vocab)\nENC_EMB_DIM = 128\nDEC_EMB_DIM = 128\nHID_DIM = 256\nN_LAYERS = 2\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n\nmodel = Seq2Seq(enc, dec, device).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:44.986999Z","iopub.execute_input":"2023-04-10T09:39:44.987696Z","iopub.status.idle":"2023-04-10T09:39:46.108781Z","shell.execute_reply.started":"2023-04-10T09:39:44.987654Z","shell.execute_reply":"2023-04-10T09:39:46.107554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing the model parameters\nDistribution used is taken from the github notebook(replicated the approach mentioned in the paper)","metadata":{}},{"cell_type":"code","source":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:46.111157Z","iopub.execute_input":"2023-04-10T09:39:46.111546Z","iopub.status.idle":"2023-04-10T09:39:46.121760Z","shell.execute_reply.started":"2023-04-10T09:39:46.111507Z","shell.execute_reply":"2023-04-10T09:39:46.120618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:46.123274Z","iopub.execute_input":"2023-04-10T09:39:46.124501Z","iopub.status.idle":"2023-04-10T09:39:46.131845Z","shell.execute_reply.started":"2023-04-10T09:39:46.124460Z","shell.execute_reply":"2023-04-10T09:39:46.130795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing optimizer and loss functions","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters())\n\nTRG_PAD_IDX = fr_vocab['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:48.701597Z","iopub.execute_input":"2023-04-10T09:39:48.702232Z","iopub.status.idle":"2023-04-10T09:39:48.708009Z","shell.execute_reply.started":"2023-04-10T09:39:48.702189Z","shell.execute_reply":"2023-04-10T09:39:48.706899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train method","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, clip):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for _, (src, trg) in enumerate(dataloader):\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n        \n        output_dim = output.shape[-1]\n        \n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss / (len(dataloader)*BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:50.994857Z","iopub.execute_input":"2023-04-10T09:39:50.995607Z","iopub.status.idle":"2023-04-10T09:39:51.003345Z","shell.execute_reply.started":"2023-04-10T09:39:50.995564Z","shell.execute_reply":"2023-04-10T09:39:51.002193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation method - no gradient calculation","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n    \n        for _, (src, trg) in enumerate(dataloader):\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n            \n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n            \n            epoch_loss += loss.item()\n        \n    return epoch_loss / (len(dataloader)*BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:52.373347Z","iopub.execute_input":"2023-04-10T09:39:52.373725Z","iopub.status.idle":"2023-04-10T09:39:52.382848Z","shell.execute_reply.started":"2023-04-10T09:39:52.373691Z","shell.execute_reply":"2023-04-10T09:39:52.380818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:53.020297Z","iopub.execute_input":"2023-04-10T09:39:53.021026Z","iopub.status.idle":"2023-04-10T09:39:53.026210Z","shell.execute_reply.started":"2023-04-10T09:39:53.020983Z","shell.execute_reply":"2023-04-10T09:39:53.025120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_train\ndel df_val\ndel df_test\ndel data\ndel en_tokenizer\ndel fr_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:53.287466Z","iopub.execute_input":"2023-04-10T09:39:53.288585Z","iopub.status.idle":"2023-04-10T09:39:53.702291Z","shell.execute_reply.started":"2023-04-10T09:39:53.288539Z","shell.execute_reply":"2023-04-10T09:39:53.701169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = 'enc-dec-basic1-model.pt'","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:54.178146Z","iopub.execute_input":"2023-04-10T09:39:54.178834Z","iopub.status.idle":"2023-04-10T09:39:54.184058Z","shell.execute_reply.started":"2023-04-10T09:39:54.178793Z","shell.execute_reply":"2023-04-10T09:39:54.182807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 5\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, val_loader, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), MODEL_PATH)\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:39:56.197926Z","iopub.execute_input":"2023-04-10T09:39:56.198611Z","iopub.status.idle":"2023-04-10T09:42:27.946335Z","shell.execute_reply.started":"2023-04-10T09:39:56.198570Z","shell.execute_reply":"2023-04-10T09:42:27.945018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(MODEL_PATH))\ntest_loss = evaluate(model, test_loader, criterion)\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:42:27.948497Z","iopub.execute_input":"2023-04-10T09:42:27.948894Z","iopub.status.idle":"2023-04-10T09:42:28.514531Z","shell.execute_reply.started":"2023-04-10T09:42:27.948852Z","shell.execute_reply":"2023-04-10T09:42:28.513375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}